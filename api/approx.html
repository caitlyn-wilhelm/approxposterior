

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>ApproxPosterior Class &mdash; approxposterior 0.4 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GMM Utility Functions" href="gmmUtils.html" />
    <link rel="prev" title="API" href="../api.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> approxposterior
          

          
          </a>

          
            
            
              <div class="version">
                0.4
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Approximate Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bayesopt.html">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../map.html">MAP Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/fittingALine.html">Fitting a Line</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/ScalingAccuracy.html">Scaling and Accuracy</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">approx.py</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#approx-py-approxposterior"><code class="xref py py-mod docutils literal notranslate"><span class="pre">approx.py</span></code> - ApproxPosterior</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="gmmUtils.html">gmmUtils.py</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpUtils.html">gpUtils.py</a></li>
<li class="toctree-l2"><a class="reference internal" href="mcmcUtils.html">mcmcUtils.py</a></li>
<li class="toctree-l2"><a class="reference internal" href="likelihood.html">likelihood.py</a></li>
<li class="toctree-l2"><a class="reference internal" href="utility.html">utility.py</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../citation.html">Citation</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/dflemin3/approxposterior">Github</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/dflemin3/approxposterior/issues">Submit an Issue</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">approxposterior</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../api.html">API</a> &raquo;</li>
        
      <li>ApproxPosterior Class</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/approx.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="module-approxposterior.approx">
<span id="approxposterior-class"></span><h1>ApproxPosterior Class<a class="headerlink" href="#module-approxposterior.approx" title="Permalink to this headline">¶</a></h1>
<div class="section" id="approx-py-approxposterior">
<h2><code class="xref py py-mod docutils literal notranslate"><span class="pre">approx.py</span></code> - ApproxPosterior<a class="headerlink" href="#approx-py-approxposterior" title="Permalink to this headline">¶</a></h2>
<p>Approximate Bayesian Posterior estimation and Bayesian optimzation. approxposterior
uses Dan Forman-Mackey’s Gaussian Process implementation, george, and the
Metropolis-Hastings MCMC ensemble sampler, emcee, to infer the approximate
posterior distributions given the GP model.</p>
<dl class="class">
<dt id="approxposterior.approx.ApproxPosterior">
<em class="property">class </em><code class="sig-prename descclassname">approxposterior.approx.</code><code class="sig-name descname">ApproxPosterior</code><span class="sig-paren">(</span><em class="sig-param">theta</em>, <em class="sig-param">y</em>, <em class="sig-param">lnprior</em>, <em class="sig-param">lnlike</em>, <em class="sig-param">priorSample</em>, <em class="sig-param">bounds</em>, <em class="sig-param">gp=None</em>, <em class="sig-param">algorithm='bape'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior" title="Permalink to this definition">¶</a></dt>
<dd><p>Class used to estimate approximate Bayesian posterior distributions or
perform Bayesian optimization using a Gaussian process surrogate model</p>
<p class="rubric">Methods</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.bayesOpt" title="approxposterior.approx.ApproxPosterior.bayesOpt"><code class="xref py py-obj docutils literal notranslate"><span class="pre">bayesOpt</span></code></a>(self, nmax[, theta0, tol, kmax, …])</p></td>
<td><p>Perform Bayesian optimization given a GP surrogate model to estimate</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.findMAP" title="approxposterior.approx.ApproxPosterior.findMAP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">findMAP</span></code></a>(self[, theta0, method, options, …])</p></td>
<td><p>Find the maximum a posteriori (MAP) estimate of the function learned by the GP.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.findNextPoint" title="approxposterior.approx.ApproxPosterior.findNextPoint"><code class="xref py py-obj docutils literal notranslate"><span class="pre">findNextPoint</span></code></a>(self[, theta0, computeLnLike, …])</p></td>
<td><p>Find numNewPoints new point(s), thetaT, by maximizing utility function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.optGP" title="approxposterior.approx.ApproxPosterior.optGP"><code class="xref py py-obj docutils literal notranslate"><span class="pre">optGP</span></code></a>(self[, seed, method, options, p0, …])</p></td>
<td><p>Optimize hyperparameters of approx object’s GP</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.run" title="approxposterior.approx.ApproxPosterior.run"><code class="xref py py-obj docutils literal notranslate"><span class="pre">run</span></code></a>(self[, m, nmax, seed, timing, verbose, …])</p></td>
<td><p>Core method to estimate the approximate posterior distribution via Gaussian Process regression</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#approxposterior.approx.ApproxPosterior.runMCMC" title="approxposterior.approx.ApproxPosterior.runMCMC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">runMCMC</span></code></a>(self[, samplerKwargs, mcmcKwargs, …])</p></td>
<td><p>Given forward model input-output pairs, theta and y, and a trained GP, run an MCMC using the GP to evaluate the logprobability instead of the true, computationally-expensive forward model.</p></td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.bayesOpt">
<code class="sig-name descname">bayesOpt</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">nmax</em>, <em class="sig-param">theta0=None</em>, <em class="sig-param">tol=0.001</em>, <em class="sig-param">kmax=3</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">runName='apRun'</em>, <em class="sig-param">cache=True</em>, <em class="sig-param">gpMethod='powell'</em>, <em class="sig-param">gpOptions=None</em>, <em class="sig-param">gpP0=None</em>, <em class="sig-param">optGPEveryN=1</em>, <em class="sig-param">nGPRestarts=1</em>, <em class="sig-param">nMinObjRestarts=5</em>, <em class="sig-param">initGPOpt=True</em>, <em class="sig-param">minObjMethod='nelder-mead'</em>, <em class="sig-param">gpHyperPrior=&lt;function defaultHyperPrior at 0x181f9b9680&gt;</em>, <em class="sig-param">minObjOptions=None</em>, <em class="sig-param">findMAP=True</em>, <em class="sig-param">args=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.bayesOpt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.bayesOpt" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform Bayesian optimization given a GP surrogate model to estimate</p>
<p>thetaBest = argmax(fn(theta))</p>
<p>given a GP trained on (theta, y). In this case, fn is the function
specified by self._lnlike + self._lnprior. Note that this function
<em>maximizes</em> the objective, so if performing a minimization,
define the objective as the negative of your function. See Brochu et al.
(2009) or Frazier (2018) for good reviews of Bayesian optimization.</p>
<p>This function terminates once nmax points have been selected or when
the function value changes by less than tol over consecutive iterations,
whichever one happens first.</p>
<p>Note 1: lnlike does not have to be a log likelihood, but rather can be any
continous function one wishes to optimize. The function lnprior is used
to place priors on parameters of the function, theta. The typical use
of lnprior is to ensure the solution remains within a hypercube or
simplex, i.e., bounding the possible values of theta.</p>
<p>Note 2: For this function, it is recommended to keep optGPEveryN = 1 to
ensure the GP properly learns the underlying function.</p>
<p>Note 3: Bayesian optimization and MAP estimation typically work better
when fitAmp = True, that is the GP kernel has an amplitude term</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>nmax</strong><span class="classifier">int</span></dt><dd><p>Maximum number of new design points to find. These are the
points that are selected by maximizing the utility function, e.g.
the expected improvement, and sequentially added to the GP training
set.</p>
</dd>
<dt><strong>theta0</strong><span class="classifier">iterable</span></dt><dd><p>Initial guess. Defaults to a sample from the prior function.</p>
</dd>
<dt><strong>tol</strong><span class="classifier">float (optional)</span></dt><dd><p>Convergence tolerance. This function will terminate if the function
value at the estimated extremum changes by less than tol over
kmax consecutive iterations. Defaults to 1.0e-3.</p>
</dd>
<dt><strong>kmax</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of iterations required for the difference in estimated
extremum functions values &lt; tol required for convergence. Defaults
to 3.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (optional)</span></dt><dd><p>RNG seed.  Defaults to None.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool (optional)</span></dt><dd><p>Output all the diagnostics? Defaults to True.</p>
</dd>
<dt><strong>runName</strong><span class="classifier">str (optional)</span></dt><dd><p>Filename to prepend to cache files where model input-output pairs
and the current GP hyperparameter values are saved. Defaults to
apRun.</p>
</dd>
<dt><strong>cache</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to cache forward model input-output pairs, and GP
kernel parameters.  Defaults to True since they’re
expensive to evaluate. In practice, users should cache forward model
inputs, outputs, ancillary parameters, etc in each likelihood
function evaluation, but saving theta and y here doesn’t hurt.</p>
</dd>
<dt><strong>gpMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimized GP hyperparameters.
Defaults to powell (it usually works)</p>
</dd>
<dt><strong>gpOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used to optimize GP
hyperparameters.  Defaults to None.</p>
</dd>
<dt><strong>gpP0</strong><span class="classifier">array (optional)</span></dt><dd><p>Initial guess for kernel hyperparameters.  If None, defaults to
np.random.randn for each parameter.</p>
</dd>
<dt><strong>optGPEveryN</strong><span class="classifier">int (optional)</span></dt><dd><p>How often to optimize the GP hyperparameters.  Defaults to
re-optimizing everytime a new design point is found, e.g. every time
a new (theta, y) pair is added to the training set.</p>
</dd>
<dt><strong>nGPRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart GP hyperparameter optimization.  Defaults
to 1. Increase this number if the GP is not well-optimized.</p>
</dd>
<dt><strong>nMinObjRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart minimizing -utility function to select
next point to improve GP performance.  Defaults to 5.  Increase this
number of the point selection is not working well.</p>
</dd>
<dt><strong>initGPOpt</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to optimize GP hyperparameters before 0th iteration.
Defaults to True (aka assume user didn’t optimize GP hyperparameters)</p>
</dd>
<dt><strong>gpHyperPrior</strong><span class="classifier">str/callable (optional)</span></dt><dd><p>Prior function for GP hyperparameters. Defaults to the defaultHyperPrior fn.
This function asserts that the mean must be negative and that each log
hyperparameter is within the range [-20,20].</p>
</dd>
<dt><strong>minObjMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimizing
utility functions for point selection.  Defaults to nelder-mead.</p>
</dd>
<dt><strong>minObjOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used when optimizing
utility functions for point selection.  Defaults to None,
but if method == “nelder-mead”, options = {“adaptive” : True}</p>
</dd>
<dt><strong>args</strong><span class="classifier">iterable (optional)</span></dt><dd><p>Arguments for user-specified loglikelihood function that calls the
forward model. Defaults to None.</p>
</dd>
<dt><strong>kwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Keyword arguments for user-specified loglikelihood function that
calls the forward model.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>soln</strong><span class="classifier">dict</span></dt><dd><p>Dictionary that contains the following keys: “thetaBest” : Best fit
solution, “valBest” : function value at best fit solution, thetas :
solution vector, vals : function values along solution, “nev” :
number of forward model evaluations, aka number of iterations</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.findMAP">
<code class="sig-name descname">findMAP</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">theta0=None</em>, <em class="sig-param">method='nelder-mead'</em>, <em class="sig-param">options=None</em>, <em class="sig-param">nRestarts=15</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.findMAP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.findMAP" title="Permalink to this definition">¶</a></dt>
<dd><p>Find the maximum a posteriori (MAP) estimate of the function learned
by the GP. To find the MAP, this function minimizes -mean predicted by
the GP, aka finds what the GP believes is the maximum of whatever
function is definded by self._lnlike + self._lnprior.</p>
<p>Note: MAP estimation typically work better when fitAmp = True, that is
the GP kernel fits for an amplitude term.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>theta0</strong><span class="classifier">iterable</span></dt><dd><p>Initial guess. Defaults to a sample from the prior function.</p>
</dd>
<dt><strong>method</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method.  Defaults to powell.</p>
</dd>
<dt><strong>options</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function.  Defaults to None.</p>
</dd>
<dt><strong>nRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart the optimization. Defaults to 15.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>MAP</strong><span class="classifier">iterable</span></dt><dd><p>maximum a posteriori estimate</p>
</dd>
<dt><strong>MAPVal</strong><span class="classifier">float</span></dt><dd><p>Mean of GP predictive function at MAP solution</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.findNextPoint">
<code class="sig-name descname">findNextPoint</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">theta0=None</em>, <em class="sig-param">computeLnLike=True</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">cache=True</em>, <em class="sig-param">gpOptions=None</em>, <em class="sig-param">gpP0=None</em>, <em class="sig-param">args=None</em>, <em class="sig-param">nGPRestarts=1</em>, <em class="sig-param">nMinObjRestarts=5</em>, <em class="sig-param">gpMethod='powell'</em>, <em class="sig-param">minObjMethod='nelder-mead'</em>, <em class="sig-param">minObjOptions=None</em>, <em class="sig-param">runName='apRun'</em>, <em class="sig-param">numNewPoints=1</em>, <em class="sig-param">optGPEveryN=1</em>, <em class="sig-param">gpHyperPrior=&lt;function defaultHyperPrior at 0x181f9b9680&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.findNextPoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.findNextPoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Find numNewPoints new point(s), thetaT, by maximizing utility function.
Note that we call a minimizer because minimizing negative of utility
function is the same as maximizing it.</p>
<dl class="simple">
<dt>This function can be used in 2 ways:</dt><dd><ol class="arabic simple">
<li><p>Finding the new point(s), thetaT, that would maximally improve the
GP’s predictive ability.  This point could be used to select
where to run a new forward model, for example.</p></li>
<li><p>Find a new thetaT and evaluate the forward model at this location
to iteratively improve the GP’s predictive performance, a core
function of the BAPE and AGP algorithms.</p></li>
</ol>
</dd>
</dl>
<p>If computeLnLike is True, all results of this function are appended to
the corresponding object elements, e.g. thetaT appended to self.theta.
thetaT is returned, as well as yT if computeLnLike is True.  Note that
returning yT requires running the forward model and updating the GP.</p>
<p>If numNewPoints &gt; 1, iteratively find numNewPoints. After each new
point is found, re-compute the GP covariance matrix. The GP
hyperparameters are then optionally re-optimized at the specified
cadence.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>theta0</strong><span class="classifier">float/iterable (optional)</span></dt><dd><p>Initial guess for optimization. Defaults to None, which draws a sample
from the prior function using sampleFn.</p>
</dd>
<dt><strong>computeLnLike</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to run the forward model and compute yT, the sum of
the lnlikelihood and lnprior. Defaults to True. If True, also
appends all new values to self.theta, self.y, in addition to
returning the new values</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (optional)</span></dt><dd><p>RNG seed.  Defaults to None.</p>
</dd>
<dt><strong>cache</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to cache forward model input-output pairs.  Defaults
to True since the forward model is expensive to evaluate. In
practice, users should cache forward model inputs, outputs,
ancillary parameters, etc in each likelihood function evaluation,
but saving theta and y here doesn’t hurt.  Saves the results to
apFModelCache.npz in the current working directory (name can change
if user specifies runName).</p>
</dd>
<dt><strong>optGPEveryN</strong><span class="classifier">int (optional)</span></dt><dd><p>How often to optimize the GP hyperparameters.  Defaults to
re-optimizing everytime a new design point is found, e.g. every time
a new (theta, y) pair is added to the training set.  Increase this
parameter if approxposterior is running slowly. NB: GP hyperparameters
are optimized <em>only</em> if computeLnLike == True</p>
</dd>
<dt><strong>gpMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimized GP hyperparameters.
Defaults to None, which is powell, and it usually works.</p>
</dd>
<dt><strong>gpOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used to optimize GP
hyperparameters.  Defaults to None.</p>
</dd>
<dt><strong>gpP0</strong><span class="classifier">array (optional)</span></dt><dd><p>Initial guess for kernel hyperparameters.  If None, defaults to
np.random.randn for each parameter.</p>
</dd>
<dt><strong>nGPRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart GP hyperparameter optimization.  Defaults
to 1. Increase this number if the GP is not well-optimized.</p>
</dd>
<dt><strong>nMinObjRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart minimizing -utility function to select
next point to improve GP performance.  Defaults to 5.  Increase this
number of the point selection is not working well.</p>
</dd>
<dt><strong>runName</strong><span class="classifier">str (optional)</span></dt><dd><p>Filename for hdf5 file where mcmc chains are saved.  Defaults to
apRun.</p>
</dd>
<dt><strong>gpHyperPrior</strong><span class="classifier">str/callable (optional)</span></dt><dd><p>Prior function for GP hyperparameters. Defaults to the defaultHyperPrior fn.
This function asserts that the mean must be negative and that each log
hyperparameter is within the range [-20,20].</p>
</dd>
<dt><strong>numNewPoints</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of new points to find. Defaults to 1.</p>
</dd>
<dt><strong>minObjMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimizing
utility functions for point selection.  Defaults to nelder-mead.</p>
</dd>
<dt><strong>minObjOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used when optimizing
utility functions for point selection.  Defaults to None,
but if method == “nelder-mead”, options = {“adaptive” : True}</p>
</dd>
<dt><strong>args</strong><span class="classifier">iterable (optional)</span></dt><dd><p>Arguments for user-specified loglikelihood function that calls the
forward model. Defaults to None.</p>
</dd>
<dt><strong>kwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Keyword arguments for user-specified loglikelihood function that
calls the forward model. Defaults to None.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>thetaT</strong><span class="classifier">float or iterable</span></dt><dd><p>New design point(s) selected by maximizing GP utility function.</p>
</dd>
<dt><strong>yT</strong><span class="classifier">float or iterable (optional)</span></dt><dd><p>Value(s) of loglikelihood + logprior at thetaT. Only returned if
computeLnLike == True</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.optGP">
<code class="sig-name descname">optGP</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">method='powell'</em>, <em class="sig-param">options=None</em>, <em class="sig-param">p0=None</em>, <em class="sig-param">nGPRestarts=1</em>, <em class="sig-param">gpHyperPrior=&lt;function defaultHyperPrior at 0x181f9b9680&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.optGP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.optGP" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimize hyperparameters of approx object’s GP</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>seed</strong><span class="classifier">int (optional)</span></dt><dd><p>numpy RNG seed.  Defaults to None.</p>
</dd>
<dt><strong>nGPRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart GP hyperparameter optimization.  Defaults
to 1. Increase this number if the GP is not well-optimized.</p>
</dd>
<dt><strong>method</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method.  Defaults to powell.</p>
</dd>
<dt><strong>options</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function.  Defaults to None.</p>
</dd>
<dt><strong>p0</strong><span class="classifier">array (optional)</span></dt><dd><p>Initial guess for kernel hyperparameters.  If None, defaults to
np.random.randn for each parameter</p>
</dd>
<dt><strong>gpHyperPrior</strong><span class="classifier">str/callable (optional)</span></dt><dd><p>Prior function for GP hyperparameters. Defaults to the defaultHyperPrior fn.
This function asserts that the mean must be negative and that each log
hyperparameter is within the range [-20,20].</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>optimizedGP</strong><span class="classifier">george.GP</span></dt><dd></dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.run">
<code class="sig-name descname">run</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">m=10</em>, <em class="sig-param">nmax=2</em>, <em class="sig-param">seed=None</em>, <em class="sig-param">timing=False</em>, <em class="sig-param">verbose=True</em>, <em class="sig-param">mcmcKwargs=None</em>, <em class="sig-param">samplerKwargs=None</em>, <em class="sig-param">estBurnin=False</em>, <em class="sig-param">thinChains=False</em>, <em class="sig-param">runName='apRun'</em>, <em class="sig-param">cache=True</em>, <em class="sig-param">gpMethod='powell'</em>, <em class="sig-param">gpOptions=None</em>, <em class="sig-param">gpP0=None</em>, <em class="sig-param">optGPEveryN=1</em>, <em class="sig-param">nGPRestarts=1</em>, <em class="sig-param">nMinObjRestarts=5</em>, <em class="sig-param">onlyLastMCMC=False</em>, <em class="sig-param">initGPOpt=True</em>, <em class="sig-param">kmax=3</em>, <em class="sig-param">gpHyperPrior=&lt;function defaultHyperPrior at 0x181f9b9680&gt;</em>, <em class="sig-param">eps=1.0</em>, <em class="sig-param">convergenceCheck=False</em>, <em class="sig-param">minObjMethod='nelder-mead'</em>, <em class="sig-param">minObjOptions=None</em>, <em class="sig-param">args=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Core method to estimate the approximate posterior distribution via
Gaussian Process regression</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>m</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of new design points to find each iteration. These are the
points that are selected by maximizing the utility function, e.g.
bape or agp, and sequentially added to the GP training set.  Defaults
to 10.</p>
</dd>
<dt><strong>nmax</strong><span class="classifier">int (optional)</span></dt><dd><p>Maximum number of iterations.  Defaults to 2. Algorithm will terminate
if either nmax iterations is met or the convergence criterion is met
if convergenceCheck is True.</p>
</dd>
<dt><strong>seed</strong><span class="classifier">int (optional)</span></dt><dd><p>RNG seed.  Defaults to None.</p>
</dd>
<dt><strong>timing</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to time the code for profiling/speed tests.
Defaults to False.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool (optional)</span></dt><dd><p>Output all the diagnostics? Defaults to True.</p>
</dd>
<dt><strong>samplerKwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Parameters for emcee.EnsembleSampler object
If None, defaults to the following:</p>
<blockquote>
<div><dl class="simple">
<dt>nwalkers<span class="classifier">int (optional)</span></dt><dd><p>Number of emcee walkers.  Defaults to 10 * dim</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt><strong>mcmcKwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Parameters for emcee.EnsembleSampler.sample/.run_mcmc methods. If
None, defaults to the following required parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>iterations<span class="classifier">int (optional)</span></dt><dd><p>Number of MCMC steps.  Defaults to 10,000</p>
</dd>
<dt>initial_state<span class="classifier">array/emcee.State (optional)</span></dt><dd><p>Initial guess for MCMC walkers.  Defaults to None and
creates guess from priors.</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt><strong>estBurnin</strong><span class="classifier">bool (optional)</span></dt><dd><p>Estimate burn-in time using integrated autocorrelation time
heuristic.  Defaults to True. In general, we recommend users
inspect the chains (note that approxposterior always at least saves
the last sampler object, or all chains if cache = True) and
calculate the burnin after the fact to ensure convergence.</p>
</dd>
<dt><strong>thinChains</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to thin chains before GMM fitting.  Useful if running
long chains.  Defaults to True.  If true, estimates a thin cadence
via int(0.5*np.min(tau)) where tau is the intergrated autocorrelation
time.</p>
</dd>
<dt><strong>runName</strong><span class="classifier">str (optional)</span></dt><dd><p>Filename for hdf5 file where mcmc chains are saved.  Defaults to
apRun and will be saved as apRunii.h5 for ii in range(nmax).</p>
</dd>
<dt><strong>cache</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to cache MCMC chains, forward model input-output
pairs, and GP kernel parameters.  Defaults to True since they’re
expensive to evaluate. In practice, users should cache forward model
inputs, outputs, ancillary parameters, etc in each likelihood
function evaluation, but saving theta and y here doesn’t hurt.
Saves the forward model, results to runNameAPFModelCache.npz,
the chains as runNameii.h5 for each, iteration ii, and the GP
parameters in runNameAPGP.npz in the current working directory, etc.</p>
</dd>
<dt><strong>gpMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimized GP hyperparameters.
Defaults to powell (it usually works)</p>
</dd>
<dt><strong>gpOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used to optimize GP
hyperparameters.  Defaults to None.</p>
</dd>
<dt><strong>gpP0</strong><span class="classifier">array (optional)</span></dt><dd><p>Initial guess for kernel hyperparameters.  If None, defaults to
np.random.randn for each parameter.</p>
</dd>
<dt><strong>optGPEveryN</strong><span class="classifier">int (optional)</span></dt><dd><p>How often to optimize the GP hyperparameters.  Defaults to
re-optimizing everytime a new design point is found, e.g. every time
a new (theta, y) pair is added to the training set.  Increase this
parameter if approxposterior is running slowly.</p>
</dd>
<dt><strong>nGPRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart GP hyperparameter optimization.  Defaults
to 1. Increase this number if the GP is not well-optimized.</p>
</dd>
<dt><strong>nMinObjRestarts</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of times to restart minimizing -utility function to select
next point to improve GP performance.  Defaults to 5.  Increase this
number of the point selection is not working well.</p>
</dd>
<dt><strong>onlyLastMCMC</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to only run the MCMC last iteration. Defaults to False.</p>
</dd>
<dt><strong>initGPOpt</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to optimize GP hyperparameters before 0th iteration.
Defaults to True (aka assume user didn’t optimize GP hyperparameters)</p>
</dd>
<dt><strong>gpHyperPrior</strong><span class="classifier">str/callable (optional)</span></dt><dd><p>Prior function for GP hyperparameters. Defaults to the defaultHyperPrior fn.
This function asserts that the mean must be negative and that each log
hyperparameter is within the range [-20,20].</p>
</dd>
<dt><strong>eps</strong><span class="classifier">float (optional)</span></dt><dd><p>Change in the mean of the approximate marginal posterior
distributions, relative to the previous marginal posterior distribution’s
standard deviation (aka relative z score), for kmax iterations
required for convergence. Defaults to 1.0.</p>
</dd>
<dt><strong>kmax</strong><span class="classifier">int (optional)</span></dt><dd><p>Number of consecutive iterations for convergence check to pass before
successfully ending algorithm. Defaults to 3.</p>
</dd>
<dt><strong>convergenceCheck</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to terminate the execution if the change in the mean
of the approximate marginal posterior distributions, relative to the
previous marginal posterior distribution’s standard deviation
(aka relative z score) varies by less than eps for kmax consecutive
iterations. Defaults to False. Note: if using this, make sure you’re
confortable with the burnin and thinning applied to the MCMC chains.
See estBurnin and thinChains parameters.</p>
</dd>
<dt><strong>minObjMethod</strong><span class="classifier">str (optional)</span></dt><dd><p>scipy.optimize.minimize method used when optimizing
utility functions for point selection.  Defaults to nelder-mead.</p>
</dd>
<dt><strong>minObjOptions</strong><span class="classifier">dict (optional)</span></dt><dd><p>kwargs for the scipy.optimize.minimize function used when optimizing
utility functions for point selection.  Defaults to None,
but if method == “nelder-mead”, options = {“adaptive” : True}</p>
</dd>
<dt><strong>args</strong><span class="classifier">iterable (optional)</span></dt><dd><p>Arguments for user-specified loglikelihood function that calls the
forward model. Defaults to None.</p>
</dd>
<dt><strong>kwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Keyword arguments for user-specified loglikelihood function that
calls the forward model.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="approxposterior.approx.ApproxPosterior.runMCMC">
<code class="sig-name descname">runMCMC</code><span class="sig-paren">(</span><em class="sig-param">self</em>, <em class="sig-param">samplerKwargs=None</em>, <em class="sig-param">mcmcKwargs=None</em>, <em class="sig-param">runName='apRun'</em>, <em class="sig-param">cache=True</em>, <em class="sig-param">estBurnin=True</em>, <em class="sig-param">thinChains=True</em>, <em class="sig-param">verbose=False</em>, <em class="sig-param">args=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/approxposterior/approx.html#ApproxPosterior.runMCMC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#approxposterior.approx.ApproxPosterior.runMCMC" title="Permalink to this definition">¶</a></dt>
<dd><p>Given forward model input-output pairs, theta and y, and a trained GP,
run an MCMC using the GP to evaluate the logprobability instead of the
true, computationally-expensive forward model.</p>
<dl class="field-list">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><dl>
<dt><strong>samplerKwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Parameters for emcee.EnsembleSampler object
If None, defaults to the following:</p>
<blockquote>
<div><dl class="simple">
<dt>nwalkers<span class="classifier">int (optional)</span></dt><dd><p>Number of emcee walkers.  Defaults to 10 * dim</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt><strong>mcmcKwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Parameters for emcee.EnsembleSampler.sample/.run_mcmc methods. If
None, defaults to the following required parameters:</p>
<blockquote>
<div><dl class="simple">
<dt>iterations<span class="classifier">int (optional)</span></dt><dd><p>Number of MCMC steps.  Defaults to 10,000</p>
</dd>
<dt>initial_state<span class="classifier">array/emcee.State (optional)</span></dt><dd><p>Initial guess for MCMC walkers.  Defaults to None and
creates guess from priors.</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt><strong>runName</strong><span class="classifier">str (optional)</span></dt><dd><p>Filename prefix for all cached files, e.g. for hdf5 file where mcmc
chains are saved.  Defaults to runNameii.h5. where ii is the
current iteration number.</p>
</dd>
<dt><strong>cache</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to cache MCMC chains, forward model input-output
pairs, and GP kernel parameters.  Defaults to True since they’re
expensive to evaluate. In practice, users should cache forward model
inputs, outputs, ancillary parameters, etc in each likelihood
function evaluation, but saving theta and y here doesn’t hurt.
Saves the forward model, results to runNameAPFModelCache.npz,
the chains as runNameii.h5 for each, iteration ii, and the GP
parameters in runNameAPGP.npz in the current working directory, etc.</p>
</dd>
<dt><strong>estBurnin</strong><span class="classifier">bool (optional)</span></dt><dd><p>Estimate burn-in time using integrated autocorrelation time
heuristic.  Defaults to True. In general, we recommend users
inspect the chains and calculate the burnin after the fact to ensure
convergence, but this function works pretty well.</p>
</dd>
<dt><strong>thinChains</strong><span class="classifier">bool (optional)</span></dt><dd><p>Whether or not to thin chains before GMM fitting.  Useful if running
long chains.  Defaults to True.  If true, estimates a thin cadence
via int(0.5*np.min(tau)) where tau is the intergrated autocorrelation
time.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">bool (optional)</span></dt><dd><p>Output all the diagnostics? Defaults to False.</p>
</dd>
<dt><strong>args</strong><span class="classifier">iterable (optional)</span></dt><dd><p>Arguments for user-specified loglikelihood function that calls the
forward model. Defaults to None.</p>
</dd>
<dt><strong>kwargs</strong><span class="classifier">dict (optional)</span></dt><dd><p>Keyword arguments for user-specified loglikelihood function that
calls the forward model.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>sampler</strong><span class="classifier">emcee.EnsembleSampler</span></dt><dd><p>emcee sampler object</p>
</dd>
<dt><strong>iburn</strong><span class="classifier">int</span></dt><dd><p>burn-in index estimate.  If estBurnin == False, returns 0.</p>
</dd>
<dt><strong>ithin</strong><span class="classifier">int</span></dt><dd><p>thin cadence estimate.  If thinChains == False, returns 1.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gmmUtils.html" class="btn btn-neutral float-right" title="GMM Utility Functions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../api.html" class="btn btn-neutral float-left" title="API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, David P. Fleming

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>